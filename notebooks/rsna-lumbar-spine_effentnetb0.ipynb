{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":71549,"databundleVersionId":8561470,"sourceType":"competition"},{"sourceId":9328457,"sourceType":"datasetVersion","datasetId":5651778}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport pydicom\nimport cv2\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint","metadata":{"_uuid":"769f3a6d-d073-488f-95f6-8c34c46fdcb5","_cell_guid":"e4cc7d10-d5ba-4a3a-bc15-2602a8a9e9ad","collapsed":false,"execution":{"iopub.status.busy":"2024-09-11T18:33:09.778870Z","iopub.execute_input":"2024-09-11T18:33:09.779596Z","iopub.status.idle":"2024-09-11T18:33:20.224551Z","shell.execute_reply.started":"2024-09-11T18:33:09.779550Z","shell.execute_reply":"2024-09-11T18:33:20.223663Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train  = pd.read_csv('/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train.csv')\nlabel = pd.read_csv('/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train_label_coordinates.csv')\ntrain_desc = pd.read_csv('/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train_series_descriptions.csv')\ntest_desc = pd.read_csv('/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/test_series_descriptions.csv')","metadata":{"_uuid":"45ae8b8e-2e99-42f9-98d6-da96318acbaf","_cell_guid":"81d8ce9c-160e-4af6-9209-17fa6874f3a2","collapsed":false,"execution":{"iopub.status.busy":"2024-09-11T18:33:20.226618Z","iopub.execute_input":"2024-09-11T18:33:20.227233Z","iopub.status.idle":"2024-09-11T18:33:20.380714Z","shell.execute_reply.started":"2024-09-11T18:33:20.227194Z","shell.execute_reply":"2024-09-11T18:33:20.379828Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preview the data\ntrain.head()","metadata":{"_uuid":"d01ff453-8a71-4f8b-9818-44a398a9fd88","_cell_guid":"6e45dd1e-718d-4a87-8f90-7a08bfc23f0f","collapsed":false,"execution":{"iopub.status.busy":"2024-09-11T18:33:20.381992Z","iopub.execute_input":"2024-09-11T18:33:20.382356Z","iopub.status.idle":"2024-09-11T18:33:20.417778Z","shell.execute_reply.started":"2024-09-11T18:33:20.382319Z","shell.execute_reply":"2024-09-11T18:33:20.417028Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_desc.head()","metadata":{"_uuid":"b95b96b1-4802-46b1-9fd7-b8913599180d","_cell_guid":"40d691f4-e99e-48f7-a8a2-c6ece549a180","collapsed":false,"execution":{"iopub.status.busy":"2024-09-11T18:33:20.419766Z","iopub.execute_input":"2024-09-11T18:33:20.420065Z","iopub.status.idle":"2024-09-11T18:33:20.429384Z","shell.execute_reply.started":"2024-09-11T18:33:20.420032Z","shell.execute_reply":"2024-09-11T18:33:20.428377Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.isnull().sum()","metadata":{"_uuid":"3e1dfadb-24b4-4934-b620-028c5721ad31","_cell_guid":"3eb7a7fe-efae-4964-b10a-4dc3ec39a8d6","collapsed":false,"execution":{"iopub.status.busy":"2024-09-11T18:33:20.430551Z","iopub.execute_input":"2024-09-11T18:33:20.431068Z","iopub.status.idle":"2024-09-11T18:33:20.447952Z","shell.execute_reply.started":"2024-09-11T18:33:20.431033Z","shell.execute_reply":"2024-09-11T18:33:20.447011Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to generate image paths based on directory structure\ndef generate_image_paths(df, data_dir):\n    image_paths = []\n    for study_id, series_id in zip(df['study_id'], df['series_id']):\n        study_dir = os.path.join(data_dir, str(study_id))\n        series_dir = os.path.join(study_dir, str(series_id))\n        images = os.listdir(series_dir)\n        image_paths.extend([os.path.join(series_dir, img) for img in images])\n    return image_paths\n\n# Generate image paths for train and test data\ntrain_image_paths = generate_image_paths(train_desc, '/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train_images')\ntest_image_paths = generate_image_paths(test_desc, '/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/test_images')","metadata":{"_uuid":"a8430031-758d-4629-a2c7-44561fbb3088","_cell_guid":"833234a3-14c2-4e7e-8f42-72b4fcad4a18","collapsed":false,"execution":{"iopub.status.busy":"2024-09-11T18:33:20.449022Z","iopub.execute_input":"2024-09-11T18:33:20.449390Z","iopub.status.idle":"2024-09-11T18:33:57.057646Z","shell.execute_reply.started":"2024-09-11T18:33:20.449356Z","shell.execute_reply":"2024-09-11T18:33:57.056760Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_desc)","metadata":{"_uuid":"b5705f60-04cb-4613-a25f-d9d628ea8d9b","_cell_guid":"4606cd62-0db1-46c2-a9e7-982d9faae608","collapsed":false,"execution":{"iopub.status.busy":"2024-09-11T18:33:57.058833Z","iopub.execute_input":"2024-09-11T18:33:57.059171Z","iopub.status.idle":"2024-09-11T18:33:57.065307Z","shell.execute_reply.started":"2024-09-11T18:33:57.059137Z","shell.execute_reply":"2024-09-11T18:33:57.064345Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_image_paths)","metadata":{"_uuid":"39e04997-22e0-449f-845e-c617a87fefe4","_cell_guid":"5ac598e2-0d77-4d99-81a2-af11d4fd0d81","collapsed":false,"execution":{"iopub.status.busy":"2024-09-11T18:33:57.066745Z","iopub.execute_input":"2024-09-11T18:33:57.067150Z","iopub.status.idle":"2024-09-11T18:33:57.076605Z","shell.execute_reply.started":"2024-09-11T18:33:57.067068Z","shell.execute_reply":"2024-09-11T18:33:57.075679Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reshape_dataframe(df):\n    # Create a list of columns to exclude\n    exclude_columns = ['study_id', 'series_id', 'instance_number', 'x', 'y', 'series_description']\n    \n    # Filter the columns to process\n    columns_to_process = [col for col in df.columns if col not in exclude_columns]\n    \n    # Split the columns into condition and level, extract severity, and concatenate to form the new DataFrame\n    reshaped_df = pd.DataFrame([\n        {\n            'study_id': row['study_id'],\n            'condition': ' '.join([word.capitalize() for word in col.split('_')[:-2]]),\n            'level': col.split('_')[-2].capitalize() + '/' + col.split('_')[-1].capitalize(),\n            'severity': row[col]\n        }\n        for _, row in df.iterrows()\n        for col in columns_to_process\n    ])\n    \n    return reshaped_df\n\n# Reshape the DataFrame\nnew_train_df = reshape_dataframe(train)\n\n# Display the first few rows of the reshaped DataFrame\nnew_train_df.head()","metadata":{"_uuid":"a452b332-b05a-48e8-bee5-c5fad1b4c95f","_cell_guid":"e903bd74-eb3d-4d47-81eb-9599e0147f23","collapsed":false,"execution":{"iopub.status.busy":"2024-09-11T18:33:57.077784Z","iopub.execute_input":"2024-09-11T18:33:57.078075Z","iopub.status.idle":"2024-09-11T18:33:57.917631Z","shell.execute_reply.started":"2024-09-11T18:33:57.078044Z","shell.execute_reply":"2024-09-11T18:33:57.916698Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print columns in a neat way\nprint(\"\\nColumns in new_train_df:\")\nprint(\",\".join(new_train_df.columns))\n\nprint(\"\\nColumns in label:\")\nprint(\",\".join(label.columns))\n\nprint(\"\\nColumns in test_desc:\")\nprint(\",\".join(test_desc.columns))","metadata":{"_uuid":"cdd5e746-b060-47f8-afdf-fee183c657e2","_cell_guid":"55c084b2-334f-4753-9fdd-c31fff84dd45","collapsed":false,"execution":{"iopub.status.busy":"2024-09-11T18:33:57.921218Z","iopub.execute_input":"2024-09-11T18:33:57.921552Z","iopub.status.idle":"2024-09-11T18:33:57.927895Z","shell.execute_reply.started":"2024-09-11T18:33:57.921512Z","shell.execute_reply":"2024-09-11T18:33:57.926804Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merge the dataframes on the common columns\nmerged_df = pd.merge(new_train_df, label, on=['study_id', 'condition', 'level'], how='inner')\n# Merge the dataframes on the common column 'series_id'\nfinal_merged_df = pd.merge(merged_df, train_desc, on='series_id', how='inner')","metadata":{"_uuid":"ac6f149d-068a-4ae7-afde-5de2d8f7188e","_cell_guid":"ab32bc07-76c5-4c4c-a95a-c69622f08f93","collapsed":false,"execution":{"iopub.status.busy":"2024-09-11T18:33:57.929242Z","iopub.execute_input":"2024-09-11T18:33:57.929617Z","iopub.status.idle":"2024-09-11T18:33:58.014697Z","shell.execute_reply.started":"2024-09-11T18:33:57.929584Z","shell.execute_reply":"2024-09-11T18:33:58.013851Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merge the dataframes on the common column 'series_id'\nfinal_merged_df = pd.merge(merged_df, train_desc, on=['series_id','study_id'], how='inner')\n# Display the first few rows of the final merged dataframe\nfinal_merged_df.head()","metadata":{"_uuid":"a876f3a2-e86b-45f5-908f-49f7d58b8f02","_cell_guid":"456408f9-ce58-4cc8-bc12-b5d5b33d5315","collapsed":false,"execution":{"iopub.status.busy":"2024-09-11T18:33:58.016163Z","iopub.execute_input":"2024-09-11T18:33:58.016978Z","iopub.status.idle":"2024-09-11T18:33:58.047482Z","shell.execute_reply.started":"2024-09-11T18:33:58.016923Z","shell.execute_reply":"2024-09-11T18:33:58.046472Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_merged_df[final_merged_df['study_id'] == 4003253].sort_values(['x','y'],ascending = True)","metadata":{"_uuid":"3f71ee96-1d26-4e02-94b9-2f619edc8c57","_cell_guid":"4dfc66d2-65ce-4ad4-9bc7-797695647429","collapsed":false,"execution":{"iopub.status.busy":"2024-09-11T18:33:58.048990Z","iopub.execute_input":"2024-09-11T18:33:58.049681Z","iopub.status.idle":"2024-09-11T18:33:58.072530Z","shell.execute_reply.started":"2024-09-11T18:33:58.049634Z","shell.execute_reply":"2024-09-11T18:33:58.071640Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_merged_df[final_merged_df['series_id'] == 2448190387].sort_values(\"instance_number\")","metadata":{"_uuid":"94a7b877-ecd4-4ffd-9393-550519b36840","_cell_guid":"70fa088c-3001-4a5f-bdd2-dc729cda7dfc","collapsed":false,"execution":{"iopub.status.busy":"2024-09-11T18:33:58.073546Z","iopub.execute_input":"2024-09-11T18:33:58.073820Z","iopub.status.idle":"2024-09-11T18:33:58.090430Z","shell.execute_reply.started":"2024-09-11T18:33:58.073790Z","shell.execute_reply":"2024-09-11T18:33:58.089433Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filter the dataframe for the given study_id and sort by instance_number\nfiltered_df = final_merged_df[final_merged_df['study_id'] == 4003253].sort_values(\"instance_number\")\n\n# Display the resulting dataframe\nfiltered_df","metadata":{"_uuid":"c36d4a95-0b76-45db-af8d-08ebc0c63e8a","_cell_guid":"f54247e1-e0cd-4674-9687-650bbbeb7e78","collapsed":false,"execution":{"iopub.status.busy":"2024-09-11T18:33:58.091797Z","iopub.execute_input":"2024-09-11T18:33:58.092198Z","iopub.status.idle":"2024-09-11T18:33:58.115318Z","shell.execute_reply.started":"2024-09-11T18:33:58.092151Z","shell.execute_reply":"2024-09-11T18:33:58.114413Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sort final_merged_df by study_id, series_id, and series_description\nsorted_final_merged_df = final_merged_df[final_merged_df['study_id'] == 4003253].sort_values(by=['series_id', 'series_description', 'instance_number'])\nsorted_final_merged_df","metadata":{"_uuid":"ea890ba6-45a8-4150-b278-0acf20a589b8","_cell_guid":"2b899943-34e9-4692-a4ba-9442fcff47d1","collapsed":false,"execution":{"iopub.status.busy":"2024-09-11T18:33:58.116508Z","iopub.execute_input":"2024-09-11T18:33:58.116833Z","iopub.status.idle":"2024-09-11T18:33:58.139864Z","shell.execute_reply.started":"2024-09-11T18:33:58.116800Z","shell.execute_reply":"2024-09-11T18:33:58.138834Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In medical imaging for spinal conditions, specific MRI sequences are often used to identify different types of spinal stenosis:\n\n- **Sagittal T1-weighted images** are primarily utilized to evaluate **Neural Foraminal Narrowing**. \n- **Axial T2-weighted images** are crucial for assessing **Subarticular Stenosis**. \n- **Sagittal T2-weighted or STIR (Short Tau Inversion Recovery) images** are typically employed to detect and analyze **Spinal Canal Stenosis**.\n\nThese imaging sequences are chosen for their ability to provide the most relevant anatomical and pathological information for each specific type of stenosis.","metadata":{"_uuid":"2ab51b9b-5811-4655-a7d3-982ce5eccfec","_cell_guid":"390329b2-ad78-4dce-9961-5f1230f92935","trusted":true}},{"cell_type":"code","source":"# Create the row_id column\nfinal_merged_df['row_id'] = (\n    final_merged_df['study_id'].astype(str) + '_' +\n    final_merged_df['condition'].str.lower().str.replace(' ', '_') + '_' +\n    final_merged_df['level'].str.lower().str.replace('/', '_')\n)\n\n# Create the image_path column\nfinal_merged_df['image_path'] = (\n    '/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train_images/' + \n    final_merged_df['study_id'].astype(str) + '/' +\n    final_merged_df['series_id'].astype(str) + '/' +\n    final_merged_df['instance_number'].astype(str) + '.dcm'\n)\n\n# Note: Check image path, since there's 1 instance id, for 1 image, but there's many more images other than the ones labelled in the instance ID. \n\n# Display the updated dataframe\nfinal_merged_df.head()","metadata":{"_uuid":"337a3e61-c05b-4511-aa53-3624ae3ffe93","_cell_guid":"aafc3ed4-b4d1-4b1d-adb1-3bbb3d4a4306","collapsed":false,"execution":{"iopub.status.busy":"2024-09-11T18:33:58.141467Z","iopub.execute_input":"2024-09-11T18:33:58.141901Z","iopub.status.idle":"2024-09-11T18:33:58.377496Z","shell.execute_reply.started":"2024-09-11T18:33:58.141853Z","shell.execute_reply":"2024-09-11T18:33:58.376509Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_merged_df[final_merged_df[\"severity\"] == \"Normal/Mild\"].value_counts().sum()","metadata":{"_uuid":"696c21a3-a5fa-472d-b516-9577926025f9","_cell_guid":"7365e249-0a8f-4304-b59b-f0744faded27","collapsed":false,"execution":{"iopub.status.busy":"2024-09-11T18:33:58.378576Z","iopub.execute_input":"2024-09-11T18:33:58.378851Z","iopub.status.idle":"2024-09-11T18:33:58.533232Z","shell.execute_reply.started":"2024-09-11T18:33:58.378821Z","shell.execute_reply":"2024-09-11T18:33:58.532247Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_merged_df[final_merged_df[\"severity\"] == \"Moderate\"].value_counts().sum()","metadata":{"_uuid":"a3300099-0995-441f-8952-358e4466a99f","_cell_guid":"382bf17c-3e3e-4624-b50e-054df046f3ef","collapsed":false,"execution":{"iopub.status.busy":"2024-09-11T18:33:58.534366Z","iopub.execute_input":"2024-09-11T18:33:58.534683Z","iopub.status.idle":"2024-09-11T18:33:58.586998Z","shell.execute_reply.started":"2024-09-11T18:33:58.534650Z","shell.execute_reply":"2024-09-11T18:33:58.586018Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the base path for test images\nbase_path = '/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/test_images'\n\n# Function to get image paths for a series\ndef get_image_paths(row):\n    series_path = os.path.join(base_path, str(row['study_id']), str(row['series_id']))\n    if os.path.exists(series_path):\n        return [os.path.join(series_path, f) for f in os.listdir(series_path) if os.path.isfile(os.path.join(series_path, f))]\n    return []\n\n# Mapping of series_description to conditions\ncondition_mapping = {\n    'Sagittal T1': {'left': 'left_neural_foraminal_narrowing', 'right': 'right_neural_foraminal_narrowing'},\n    'Axial T2': {'left': 'left_subarticular_stenosis', 'right': 'right_subarticular_stenosis'},\n    'Sagittal T2/STIR': 'spinal_canal_stenosis'\n}\n\n# Create a list to store the expanded rows\nexpanded_rows = []\n\n# Expand the dataframe by adding new rows for each file path\nfor index, row in test_desc.iterrows():\n    image_paths = get_image_paths(row)\n    conditions = condition_mapping.get(row['series_description'], {})\n    if isinstance(conditions, str):  # Single condition\n        conditions = {'left': conditions, 'right': conditions}\n    for side, condition in conditions.items():\n        for image_path in image_paths:\n            expanded_rows.append({\n                'study_id': row['study_id'],\n                'series_id': row['series_id'],\n                'series_description': row['series_description'],\n                'image_path': image_path,\n                'condition': condition,\n                'row_id': f\"{row['study_id']}_{condition}\"\n            })\n\n# Create a new dataframe from the expanded rows\nexpanded_test_desc = pd.DataFrame(expanded_rows)\n\n# Display the resulting dataframe\nexpanded_test_desc.head(5)","metadata":{"_uuid":"b03c8c55-40a7-4df1-a7c6-4c3c542eefc9","_cell_guid":"7897c7f7-85d6-46cc-85db-cec4c363eee4","collapsed":false,"execution":{"iopub.status.busy":"2024-09-11T18:33:58.588138Z","iopub.execute_input":"2024-09-11T18:33:58.588467Z","iopub.status.idle":"2024-09-11T18:33:58.615767Z","shell.execute_reply.started":"2024-09-11T18:33:58.588432Z","shell.execute_reply":"2024-09-11T18:33:58.614811Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Replace null values in the 'severity' column with 'Normal/Mild'\nfinal_merged_df['severity'].fillna('Normal/Mild', inplace=True)","metadata":{"_uuid":"596d6d58-0014-4adb-8147-3e3b93786fd6","_cell_guid":"d98e8037-d2d7-4025-834f-c4119ec2b08e","collapsed":false,"execution":{"iopub.status.busy":"2024-09-11T18:33:58.617049Z","iopub.execute_input":"2024-09-11T18:33:58.617497Z","iopub.status.idle":"2024-09-11T18:33:58.628475Z","shell.execute_reply.started":"2024-09-11T18:33:58.617449Z","shell.execute_reply":"2024-09-11T18:33:58.627542Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = expanded_test_desc\ntrain_data = final_merged_df","metadata":{"_uuid":"c7af545f-d5e5-4eb2-994d-cb97848b8546","_cell_guid":"7e44f186-126c-4919-8f11-2761c8b06bf1","collapsed":false,"execution":{"iopub.status.busy":"2024-09-11T18:33:58.629657Z","iopub.execute_input":"2024-09-11T18:33:58.629983Z","iopub.status.idle":"2024-09-11T18:33:58.637341Z","shell.execute_reply.started":"2024-09-11T18:33:58.629943Z","shell.execute_reply":"2024-09-11T18:33:58.636535Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.isnull().sum()","metadata":{"_uuid":"05140088-d100-4f96-9b65-edb0eec7ae23","_cell_guid":"23dd5387-eac5-4dc4-afe1-6a41da1b85c2","collapsed":false,"execution":{"iopub.status.busy":"2024-09-11T18:33:58.638613Z","iopub.execute_input":"2024-09-11T18:33:58.639024Z","iopub.status.idle":"2024-09-11T18:33:58.679796Z","shell.execute_reply.started":"2024-09-11T18:33:58.638981Z","shell.execute_reply":"2024-09-11T18:33:58.678729Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data","metadata":{"_uuid":"2322971c-ef48-45ad-8cbf-48babc045096","_cell_guid":"83acb2ac-89c0-4164-87ce-ea0682a9b6c6","collapsed":false,"execution":{"iopub.status.busy":"2024-09-11T18:33:58.680924Z","iopub.execute_input":"2024-09-11T18:33:58.681285Z","iopub.status.idle":"2024-09-11T18:33:58.699100Z","shell.execute_reply.started":"2024-09-11T18:33:58.681243Z","shell.execute_reply":"2024-09-11T18:33:58.697947Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checking whether there are any errors or outliers in the co-ordinates that are necessary to remove or not","metadata":{"_uuid":"cb3aadcc-9db1-4008-a386-7ed28cf483d0","_cell_guid":"53a3cb20-8556-402b-80f7-935023f7af39","trusted":true}},{"cell_type":"markdown","source":"# Data Visualizations","metadata":{"_uuid":"d98a81e1-8d6f-4d7f-95a3-3b280d0cf1a7","_cell_guid":"68ddfae7-7959-42c3-a1a6-edece55fdceb","trusted":true}},{"cell_type":"code","source":"# Display basic statistics for 'x' and 'y' columns\nx_stats = train_data['x'].describe()\ny_stats = train_data['y'].describe()\n\nprint(\"X Coordinate Statistics:\")\nprint(x_stats)\n\nprint(\"\\nY Coordinate Statistics:\")\nprint(y_stats)","metadata":{"_uuid":"b9428fa6-02bf-45e2-97c0-365516a8faa6","_cell_guid":"74229ce7-9bea-48f6-8ee8-bd2e37371534","collapsed":false,"execution":{"iopub.status.busy":"2024-09-11T18:33:58.700335Z","iopub.execute_input":"2024-09-11T18:33:58.700680Z","iopub.status.idle":"2024-09-11T18:33:58.719604Z","shell.execute_reply.started":"2024-09-11T18:33:58.700646Z","shell.execute_reply":"2024-09-11T18:33:58.718670Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n# Create a histogram for 'x' values\nx_hist = go.Histogram(\n    x=train_data['x'],\n    nbinsx=30,\n    name='X Coordinates',\n    marker_color='blue',\n    opacity=0.7\n)\n\n# Create a histogram for 'y' values\ny_hist = go.Histogram(\n    x=train_data['y'],\n    nbinsx=30,\n    name='Y Coordinates',\n    marker_color='green',\n    opacity=0.7\n)\n\n# Create a figure with subplots\nfig = make_subplots(rows=1, cols=2, subplot_titles=('Distribution of X Coordinates', 'Distribution of Y Coordinates'))\n\n# Add the histograms to the figure\nfig.add_trace(x_hist, row=1, col=1)\nfig.add_trace(y_hist, row=1, col=2)\n\n# Update layout for a cleaner look\nfig.update_layout(\n    title_text=\"Distribution of X and Y Coordinates\",\n    showlegend=False,\n    xaxis_title=\"X Values\",\n    yaxis_title=\"Frequency\",\n    xaxis2_title=\"Y Values\",\n    yaxis2_title=\"Frequency\",\n    bargap=0.2,  # Gap between bars\n)\n\n# Show the plot\nfig.show()","metadata":{"_uuid":"0e6adc83-237d-42ee-aeb6-7b5e6763762a","_cell_guid":"d98c3da8-50e1-49f4-ae1e-8e710301c486","collapsed":false,"execution":{"iopub.status.busy":"2024-09-11T18:33:58.721074Z","iopub.execute_input":"2024-09-11T18:33:58.721748Z","iopub.status.idle":"2024-09-11T18:33:59.398565Z","shell.execute_reply.started":"2024-09-11T18:33:58.721700Z","shell.execute_reply":"2024-09-11T18:33:59.397522Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px\n\n# Calculate the count of each condition in the 'condition' column\ncondition_counts = train_data['condition'].value_counts()\n\n# Create a bar chart using Plotly\nfig = px.bar(\n    x=condition_counts.index,\n    y=condition_counts.values,\n    labels={'x': 'Condition', 'y': 'Count'},\n    title='Count of Each Condition'\n)\n\n# Display the interactive plot\nfig.show()","metadata":{"_uuid":"05975bb4-9727-4544-903a-d757305a4815","_cell_guid":"47f031ca-0b82-4402-acb6-2b73d79d5938","collapsed":false,"execution":{"iopub.status.busy":"2024-09-11T18:33:59.400343Z","iopub.execute_input":"2024-09-11T18:33:59.400676Z","iopub.status.idle":"2024-09-11T18:34:00.214395Z","shell.execute_reply.started":"2024-09-11T18:33:59.400642Z","shell.execute_reply":"2024-09-11T18:34:00.213387Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Count the occurrences of each severity level\nseverity_counts = train_data['severity'].value_counts()\n\n# Create a bar chart using Plotly\nfig_bar = px.bar(\n    severity_counts,\n    x=severity_counts.index,\n    y=severity_counts.values,\n    labels={'index': 'Severity', 'y': 'Count'},\n    title='Count of Each Severity Level'\n)\n\n# Display the bar chart\nfig_bar.show()","metadata":{"_uuid":"3f8cc993-79fd-422b-9b29-bdfa19b603e7","_cell_guid":"2af00201-fe28-463b-a15d-2903df7e86ae","collapsed":false,"execution":{"iopub.status.busy":"2024-09-11T18:34:00.221429Z","iopub.execute_input":"2024-09-11T18:34:00.221747Z","iopub.status.idle":"2024-09-11T18:34:00.294766Z","shell.execute_reply.started":"2024-09-11T18:34:00.221715Z","shell.execute_reply":"2024-09-11T18:34:00.293753Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a pie chart using Plotly\nfig_pie = px.pie(\n    severity_counts,\n    names=severity_counts.index,\n    values=severity_counts.values,\n    title='Distribution of Severity Levels',\n    hole=0.3  # For a donut chart, otherwise remove this parameter\n)\n\n# Display the pie chart\nfig_pie.show()","metadata":{"_uuid":"966ead33-e50a-4750-9ac6-4041f5d9b16b","_cell_guid":"df705101-9e35-42df-98b6-afce9506a4bb","collapsed":false,"execution":{"iopub.status.busy":"2024-09-11T18:34:00.295921Z","iopub.execute_input":"2024-09-11T18:34:00.296253Z","iopub.status.idle":"2024-09-11T18:34:00.356862Z","shell.execute_reply.started":"2024-09-11T18:34:00.296220Z","shell.execute_reply":"2024-09-11T18:34:00.355895Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px\n\n# Count the occurrences of each severity within each condition\nseverity_condition_counts = train_data.groupby(['condition', 'severity']).size().reset_index(name='count')\n\n# Create a grouped bar chart\nfig = px.bar(\n    severity_condition_counts,\n    x='condition',\n    y='count',\n    color='severity',\n    barmode='group',\n    title='Distribution of Severities for Each Condition',\n    labels={'condition': 'Condition', 'count': 'Number of Cases', 'severity': 'Severity'},\n    color_discrete_sequence=px.colors.qualitative.Set1  # Custom color sequence\n)\n\n# Update the layout for better presentation\nfig.update_layout(\n    xaxis_title='Condition',\n    yaxis_title='Number of Cases',\n    legend_title='Severity',\n    bargap=0.15,\n    bargroupgap=0.1\n)\n\nfig.show()","metadata":{"_uuid":"76a2f293-ffe9-4344-bf76-659ded8f4e74","_cell_guid":"547bdb5d-032c-45e3-b9cc-9ff07770f012","collapsed":false,"execution":{"iopub.status.busy":"2024-09-11T18:34:00.358202Z","iopub.execute_input":"2024-09-11T18:34:00.358610Z","iopub.status.idle":"2024-09-11T18:34:00.460113Z","shell.execute_reply.started":"2024-09-11T18:34:00.358565Z","shell.execute_reply":"2024-09-11T18:34:00.459071Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Count the occurrences of each severity within each condition\nseverity_condition_counts = train_data.groupby(['condition', 'series_description']).size().reset_index(name='count')\n\n# Create a grouped bar chart\nfig = px.bar(\n    severity_condition_counts,\n    x='condition',\n    y='count',\n    color='series_description',\n    barmode='group',\n    title='Distribution of Condition for Respective Angle',\n    labels={'condition': 'Condition', 'count': 'Number of Cases', 'series_description': 'Angle of MR Image'},\n    color_discrete_sequence=px.colors.qualitative.Set1  # Custom color sequence\n)\n\n# Update the layout for better presentation\nfig.update_layout(\n    xaxis_title='Condition',\n    yaxis_title='Number of Cases',\n    legend_title='Angle',\n    bargap=0.15,\n    bargroupgap=0.1\n)\n\nfig.show()","metadata":{"_uuid":"7c5b6cf6-b860-4cc0-a882-00b144b40e96","_cell_guid":"2d053601-ff9f-4855-9d48-08c894603e99","collapsed":false,"execution":{"iopub.status.busy":"2024-09-11T18:34:00.461497Z","iopub.execute_input":"2024-09-11T18:34:00.462210Z","iopub.status.idle":"2024-09-11T18:34:00.552015Z","shell.execute_reply.started":"2024-09-11T18:34:00.462160Z","shell.execute_reply":"2024-09-11T18:34:00.551056Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In medical imaging for spinal conditions, specific MRI sequences are often used to identify different types of spinal stenosis:\n\n- **Sagittal T1-weighted images** are primarily utilized to evaluate **Neural Foraminal Narrowing**. \n- **Axial T2-weighted images** are crucial for assessing **Subarticular Stenosis**. \n- **Sagittal T2-weighted or STIR (Short Tau Inversion Recovery) images** are typically employed to detect and analyze **Spinal Canal Stenosis**.\n\nThese imaging sequences are chosen for their ability to provide the most relevant anatomical and pathological information for each specific type of stenosis.","metadata":{"_uuid":"dc2582c7-942e-49e5-a10e-f3f0d044727e","_cell_guid":"5a0331a5-d395-4fea-8f6f-24e2b872cad7","trusted":true}},{"cell_type":"code","source":"# Group by 'level' and 'condition' and count the occurrences\nlevel_condition_counts = train_data.groupby(['condition', 'level']).size().reset_index(name='count')\n\n# Create a grouped bar chart\nfig = px.bar(\n    level_condition_counts,\n    x='condition',\n    y='count',\n    color='level',\n    barmode='group',\n    title='Distribution of Levels for Each Condition',\n    labels={'condition': 'Condition', 'count': 'Number of Cases', 'level': 'Level'},\n    color_discrete_sequence=px.colors.qualitative.Set1  # Custom color sequence\n)\n\n# Update the layout for better presentation\nfig.update_layout(\n    xaxis_title='Condition',\n    yaxis_title='Number of Cases',\n    legend_title='Level',\n    bargap=0.15,\n    bargroupgap=0.1\n)\n\nfig.show()","metadata":{"_uuid":"694c40e4-9768-4dd9-b3fc-8670f0d5edb1","_cell_guid":"fa01ff2a-2ad3-48db-ab44-5b6e0b98326a","collapsed":false,"execution":{"iopub.status.busy":"2024-09-11T18:34:00.553435Z","iopub.execute_input":"2024-09-11T18:34:00.553757Z","iopub.status.idle":"2024-09-11T18:34:00.651221Z","shell.execute_reply.started":"2024-09-11T18:34:00.553724Z","shell.execute_reply":"2024-09-11T18:34:00.650162Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Count the occurrences of each level within each condition\nlevel_condition_counts = train_data.groupby(['level', 'condition']).size().reset_index(name='count')\n\n# Create a pivot table to structure the data for the heatmap\nheatmap_data = level_condition_counts.pivot(index='level', columns='condition', values='count')\n\n# Create the heatmap\nfig = px.imshow(\n    heatmap_data,\n    labels={'x': 'Condition', 'y': 'Level', 'color': 'Count'},\n    title='Heatmap of Levels by Condition',\n    color_continuous_scale='Viridis'\n)\n\nfig.show()","metadata":{"_uuid":"1ad5c50f-ca9c-4f64-ba8b-4887bf63efb0","_cell_guid":"3ed6c023-4da8-47cb-a8ad-534a61157724","collapsed":false,"execution":{"iopub.status.busy":"2024-09-11T18:34:00.652593Z","iopub.execute_input":"2024-09-11T18:34:00.652994Z","iopub.status.idle":"2024-09-11T18:34:00.745235Z","shell.execute_reply.started":"2024-09-11T18:34:00.652949Z","shell.execute_reply":"2024-09-11T18:34:00.744243Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n# List of unique levels and conditions\nlevels = level_condition_counts['level'].unique()\nconditions = level_condition_counts['condition'].unique()\n\n# Define a color scale for the conditions\ncolors = px.colors.qualitative.Plotly\n\n# Create a subplot figure with 1 row for each level\nfig = make_subplots(\n    rows=len(levels), cols=1,\n    subplot_titles=[f\"Condition Distribution for {level}\" for level in levels],\n    shared_xaxes=True,\n    vertical_spacing=0.1\n)\n\n# Add a bar chart for each level in a separate subplot\nfor i, level in enumerate(levels):\n    # Filter data for the current level\n    level_data = level_condition_counts[level_condition_counts['level'] == level]\n    \n    # Create the bar chart for each condition within the level\n    for j, condition in enumerate(conditions):\n        condition_data = level_data[level_data['condition'] == condition]\n        bar = go.Bar(\n            x=condition_data['condition'],\n            y=condition_data['count'],\n            name=condition if i == 0 else None,  # Only name bars in the first subplot\n            showlegend=(i == 0),  # Only show legend items in the first subplot\n            marker=dict(color=colors[j % len(colors)])\n        )\n        \n        # Add the bar chart to the subplot\n        fig.add_trace(bar, row=i+1, col=1)\n\n# Update the layout for better presentation\nfig.update_layout(\n    height=600 + 200 * len(levels),  # Adjust height based on the number of levels\n    title_text=\"Condition Distribution Across Levels\",\n    showlegend=True,  # Show the legend\n    legend_title_text='Condition',\n    xaxis_title='Condition',\n    yaxis_title='Number of Cases'\n)\n\n# Display the figure\nfig.show()","metadata":{"_uuid":"c02ecdc3-29db-443c-ba4a-6eac7ad36dae","_cell_guid":"d8443bcb-d009-42c9-8cd3-9700e84ec8b2","collapsed":false,"execution":{"iopub.status.busy":"2024-09-11T18:34:00.746385Z","iopub.execute_input":"2024-09-11T18:34:00.746672Z","iopub.status.idle":"2024-09-11T18:34:00.848131Z","shell.execute_reply.started":"2024-09-11T18:34:00.746641Z","shell.execute_reply":"2024-09-11T18:34:00.847147Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Export the DataFrame to a CSV file\nfinal_merged_df.to_csv('train_processed.csv', index=False)\ntest_data.to_csv('test_processed.csv', index=False)","metadata":{"_uuid":"a62a3e46-b1bb-463e-a524-35320211c967","_cell_guid":"003c6c07-be21-418b-96a0-7d4c8016526d","collapsed":false,"execution":{"iopub.status.busy":"2024-09-11T18:34:00.849580Z","iopub.execute_input":"2024-09-11T18:34:00.850351Z","iopub.status.idle":"2024-09-11T18:34:01.624762Z","shell.execute_reply.started":"2024-09-11T18:34:00.850291Z","shell.execute_reply":"2024-09-11T18:34:01.623890Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Setup & Data Loading","metadata":{"_uuid":"e4ca6042-524d-4cf9-a8c2-5037d4f08bd6","_cell_guid":"96bfc540-b887-4d2d-a173-f2381f8ec3c9","trusted":true}},{"cell_type":"code","source":"# Install EfficientNet\n!pip install -U efficientnet\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nimport efficientnet.tfkeras as efn\nimport pydicom\nimport matplotlib.pyplot as plt\nimport os","metadata":{"_uuid":"ac0cca93-9a9b-4d78-862f-0083bc830a3a","_cell_guid":"51402fb3-7cfd-4fab-83ad-182d7b1835c5","collapsed":false,"execution":{"iopub.status.busy":"2024-09-11T18:34:01.626123Z","iopub.execute_input":"2024-09-11T18:34:01.626540Z","iopub.status.idle":"2024-09-11T18:34:16.401023Z","shell.execute_reply.started":"2024-09-11T18:34:01.626496Z","shell.execute_reply":"2024-09-11T18:34:16.399860Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualize Samples","metadata":{"_uuid":"99b2b4cd-74a4-4fbe-88cf-7d89afe0d1c2","_cell_guid":"6a26927f-9cca-4d99-bb45-5b2f5796d113","trusted":true}},{"cell_type":"code","source":"# Function to visualize samples\ndef visualize_samples(image_dir, filenames):\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n    \n    for i, file in enumerate(filenames):\n        ds = pydicom.dcmread(os.path.join(image_dir, file))\n        img = ds.pixel_array\n        \n        axes[i].imshow(img, cmap='gray')\n        axes[i].axis('off')\n        axes[i].set_title(f\"Sample {i+1}\")\n    \n    plt.show()\n\n# Provide filenames of different types of images\nsagittal_t1_sample = \"/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train_images/100206310/2092806862/4.dcm\"\nsagittal_t2_stir_sample = \"/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train_images/100206310/1792451510/9.dcm\"\naxial_view_sample = \"/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train_images/100206310/1012284084/55.dcm\"\n\n# Visualize the provided sample images\nvisualize_samples('images', [sagittal_t1_sample, sagittal_t2_stir_sample, axial_view_sample])","metadata":{"_uuid":"704545b2-4d62-44a8-8874-98389a96ed19","_cell_guid":"cfa02d82-9b0e-4c7c-8883-95d339157d7e","collapsed":false,"execution":{"iopub.status.busy":"2024-09-11T18:34:16.402616Z","iopub.execute_input":"2024-09-11T18:34:16.403768Z","iopub.status.idle":"2024-09-11T18:34:16.954938Z","shell.execute_reply.started":"2024-09-11T18:34:16.403732Z","shell.execute_reply":"2024-09-11T18:34:16.953954Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Augmentation and Splitting","metadata":{"_uuid":"70f4c8ad-bb6c-4476-9b70-97ee4dba1f58","_cell_guid":"1ee02463-2cef-469a-9aa9-0bc9628860b1","trusted":true}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport pydicom\nimport cv2\nimport os\n\n# Load CSV with image paths and labels\ndf = pd.read_csv('/kaggle/working/train_processed.csv')\n\n# Check the dataframe structure\nprint(df.head())\n\n# Example of how to load a DICOM image\ndef load_dicom_image(filepath, target_size=(224, 224)):\n    dicom = pydicom.dcmread(filepath)\n    img = dicom.pixel_array\n    img = cv2.resize(img, target_size)  # Resize to target size\n    img = np.stack((img,)*3, axis=-1)  # Convert grayscale to 3-channel image\n    return img / 255.0  # Normalize pixel values\n\n# Check if a DICOM file loads correctly\ntest_image = load_dicom_image(df['image_path'].iloc[0])\nprint(test_image.shape)  # Should be (224, 224, 3)","metadata":{"_uuid":"7a8d50e8-2c20-44e4-8580-5731d71c1cdb","_cell_guid":"00d20b68-f6c6-4876-b37c-9e3a406f2235","collapsed":false,"execution":{"iopub.status.busy":"2024-09-11T18:34:16.956216Z","iopub.execute_input":"2024-09-11T18:34:16.956538Z","iopub.status.idle":"2024-09-11T18:34:17.223583Z","shell.execute_reply.started":"2024-09-11T18:34:16.956504Z","shell.execute_reply":"2024-09-11T18:34:17.222592Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Generator for Loading DICOM Images","metadata":{"_uuid":"81b93b48-f7ae-4d52-a4cd-cabc7c5a16af","_cell_guid":"851df6ff-bdc6-46b1-bf11-211460a69346","trusted":true}},{"cell_type":"code","source":"from tensorflow.keras.utils import Sequence\nfrom sklearn.preprocessing import LabelEncoder\n\n# Encode condition labels to categorical values\nlabel_encoder = LabelEncoder()\ndf['condition_encoded'] = label_encoder.fit_transform(df['condition'])\n\n# Custom data generator to load DICOM images in batches\nclass DICOMDataGenerator(Sequence):\n    def __init__(self, dataframe, batch_size, target_size=(224, 224), shuffle=True):\n        self.dataframe = dataframe\n        self.batch_size = batch_size\n        self.target_size = target_size\n        self.shuffle = shuffle\n        self.on_epoch_end()\n\n    def __len__(self):\n        return int(np.floor(len(self.dataframe) / self.batch_size))\n\n    def __getitem__(self, index):\n        batch_data = self.dataframe.iloc[index * self.batch_size:(index + 1) * self.batch_size]\n        images = np.array([load_dicom_image(filepath, self.target_size) for filepath in batch_data['image_path']])\n        \n        # One-hot encode labels ensuring 5 output classes\n        labels = np.zeros((len(batch_data), 5))  # Initialize array for 5 classes\n        batch_labels = pd.get_dummies(batch_data['condition_encoded']).values\n        labels[:, :batch_labels.shape[1]] = batch_labels  # Fill with available batch data\n        \n        return images, labels\n\n    def on_epoch_end(self):\n        if self.shuffle:\n            self.dataframe = self.dataframe.sample(frac=1).reset_index(drop=True)","metadata":{"_uuid":"9cc9f0e4-76c1-4d24-a9a9-55b6dc4944e9","_cell_guid":"2ff85d00-cd13-463b-b4c4-5a320b8afe87","collapsed":false,"execution":{"iopub.status.busy":"2024-09-11T18:34:17.225077Z","iopub.execute_input":"2024-09-11T18:34:17.225510Z","iopub.status.idle":"2024-09-11T18:34:17.251065Z","shell.execute_reply.started":"2024-09-11T18:34:17.225464Z","shell.execute_reply":"2024-09-11T18:34:17.250291Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Splitting Data into Training and Validation Sets","metadata":{"_uuid":"a7459957-b8e4-4673-a9cb-b4f37c49de54","_cell_guid":"edbb2c08-8da0-4ffc-a7ed-f98cf3dc9549","trusted":true}},{"cell_type":"code","source":"# Split data into training and validation sets\ntrain_df, val_df = train_test_split(df, test_size=0.2, stratify=df['condition_encoded'])\n\n# Create the generators\ntrain_generator = DICOMDataGenerator(train_df, batch_size=16, target_size=(224, 224))\nval_generator = DICOMDataGenerator(val_df, batch_size=16, target_size=(224, 224))","metadata":{"_uuid":"a0d8778e-1b22-468e-a447-2609c513311b","_cell_guid":"d68ac9a9-18fc-4eee-aa98-f1b58e6493b8","collapsed":false,"execution":{"iopub.status.busy":"2024-09-11T18:34:17.252468Z","iopub.execute_input":"2024-09-11T18:34:17.252871Z","iopub.status.idle":"2024-09-11T18:34:17.310052Z","shell.execute_reply.started":"2024-09-11T18:34:17.252826Z","shell.execute_reply":"2024-09-11T18:34:17.309033Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Generators for Efficient Memory Usage","metadata":{"_uuid":"e97498e8-2a45-4022-980b-908c23fd45f3","_cell_guid":"8432ec98-f891-45a7-a0e2-ea76daf47af7","trusted":true}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n# Assuming you already have data loaded (e.g., 'data' for tabular and 'image_paths' for images)\n# Example: Split the dataset into training and testing sets (image and tabular data)\ntrain_data, test_data = train_test_split(data, test_size=0.2, stratify=data['severity_encoded'], random_state=42)\n\n# Load images from file paths in your train_data and test_data (assuming image paths are stored in 'image_paths' column)\ndef load_images(image_paths, target_size=(224, 224)):\n    images = []\n    for path in image_paths:\n        img = tf.keras.preprocessing.image.load_img(path, target_size=target_size)\n        img = tf.keras.preprocessing.image.img_to_array(img) / 255.0  # Normalize pixel values\n        images.append(img)\n    return np.array(images)\n\ntrain_images = load_images(train_data['image_paths'])\ntest_images = load_images(test_data['image_paths'])\n\n# Preprocess tabular data (assuming 'x' and 'y' are tabular features in your data)\nscaler = StandardScaler()\ntrain_tabular = scaler.fit_transform(train_data[['x', 'y']])\ntest_tabular = scaler.transform(test_data[['x', 'y']])\n\n# Labels (assuming 'severity_encoded' is your target column)\ntrain_labels = train_data['severity_encoded'].values\ntest_labels = test_data['severity_encoded'].values\n\n# Now you can create the data generators using the preprocessed images, tabular data, and labels\ntrain_gen = CombinedDataGenerator(train_images, train_tabular, train_labels, batch_size=8)\nval_gen = CombinedDataGenerator(test_images, test_tabular, test_labels, batch_size=8)","metadata":{"_uuid":"23d03d80-f71b-4ab4-a9bb-9e4ac1fba794","_cell_guid":"906f3135-2fdb-42f3-a593-2118cf21be4b","collapsed":false,"execution":{"iopub.status.busy":"2024-09-11T18:36:30.776820Z","iopub.execute_input":"2024-09-11T18:36:30.777871Z","iopub.status.idle":"2024-09-11T18:36:30.839046Z","shell.execute_reply.started":"2024-09-11T18:36:30.777824Z","shell.execute_reply":"2024-09-11T18:36:30.837642Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Setup with EfficientNetB0","metadata":{"_uuid":"e6a38e49-b0ff-4b82-ab45-cca408ecf809","_cell_guid":"4ec55c08-e193-4b2b-8d01-823f0dccd9d8","trusted":true}},{"cell_type":"code","source":"from tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D, Concatenate, Flatten, Conv2D, MaxPooling2D, Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\n# --------------------- Image Branch using EfficientNetB0 ---------------------\n\n# Load the EfficientNetB0 model (pre-trained) without the top layer\nbase_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n\n# Add custom layers on top of EfficientNetB0 for image data\nx = base_model.output\nx = GlobalAveragePooling2D()(x)  # Global average pooling\nx = Dense(256, activation='relu')(x)  # Custom fully connected layer\n\n# Freeze the base model's pre-trained layers\nfor layer in base_model.layers:\n    layer.trainable = False\n\n# --------------------- Tabular Data Branch ---------------------\n\n# Define the input shape for tabular data (adjust based on your actual tabular input size)\ntabular_input = Input(shape=(train_tabular.shape[1],))\n\n# Define a simple MLP (Multilayer Perceptron) for tabular data\ny = Dense(16, activation='relu')(tabular_input)  # Dense layer for tabular data\ny = Dropout(0.5)(y)  # Dropout layer to prevent overfitting\n\n# --------------------- Combine the Branches ---------------------\n\n# Concatenate image branch (x) and tabular branch (y)\ncombined = Concatenate()([x, y])\n\n# Add more dense layers on top of the combined data\nz = Dense(128, activation='relu')(combined)  # Fully connected layer\nz = Dense(64, activation='relu')(z)\n\n# Final output layer for classification (5 classes for spinal degeneration)\npredictions = Dense(5, activation='softmax')(z)  # Output layer for 5 unique conditions\n\n# --------------------- Model Definition ---------------------\n\n# Create the final model that takes both image and tabular inputs\nmodel = Model(inputs=[base_model.input, tabular_input], outputs=predictions)\n\n# Compile the model\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# --------------------- Model Summary ---------------------\n# Show the model architecture\nmodel.summary()","metadata":{"_uuid":"33f5d0e7-5f53-4bd0-81ae-7fbdb9b8b37b","_cell_guid":"3a4238f7-2dde-426c-b424-9c18f4286490","collapsed":false,"execution":{"iopub.status.busy":"2024-09-11T18:34:18.138342Z","iopub.status.idle":"2024-09-11T18:34:18.138705Z","shell.execute_reply.started":"2024-09-11T18:34:18.138524Z","shell.execute_reply":"2024-09-11T18:34:18.138543Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Checking GPU Availability","metadata":{"_uuid":"82b0dad5-4285-4e65-a54f-f09e26fab5c2","_cell_guid":"f36ea109-eb92-4a7a-8a42-ab4f52bfb89f","trusted":true}},{"cell_type":"code","source":"import tensorflow as tf\n\n# Check GPU availability\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\ntf.config.list_physical_devices('GPU')\n\n# Enable memory growth for GPU\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(\"Memory growth enabled\")\n    except RuntimeError as e:\n        print(e)","metadata":{"_uuid":"dfbdadff-c772-480b-80fb-527d87e8e7b6","_cell_guid":"ccb9254b-4b3b-47d0-800a-be5dd9fa1586","collapsed":false,"execution":{"iopub.status.busy":"2024-09-11T18:34:18.140322Z","iopub.status.idle":"2024-09-11T18:34:18.140837Z","shell.execute_reply.started":"2024-09-11T18:34:18.140566Z","shell.execute_reply":"2024-09-11T18:34:18.140593Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Mixed Precision Training","metadata":{"_uuid":"d69de76e-659d-466b-b617-ffd41713d010","_cell_guid":"fb53cc14-c981-4ae0-aba5-844191cad075","trusted":true}},{"cell_type":"code","source":"# Enable mixed precision\nfrom tensorflow.keras import mixed_precision\nmixed_precision.set_global_policy('mixed_float16')","metadata":{"_uuid":"775cf4f8-09ca-4d1d-ac8d-3a96b6c030ba","_cell_guid":"08caec34-fe91-419d-b6b6-5d25a64a2dd2","collapsed":false,"execution":{"iopub.status.busy":"2024-09-11T18:34:18.142655Z","iopub.status.idle":"2024-09-11T18:34:18.143200Z","shell.execute_reply.started":"2024-09-11T18:34:18.142889Z","shell.execute_reply":"2024-09-11T18:34:18.142914Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Image Preprocessing","metadata":{"_uuid":"87126b70-2d8b-43d0-a9ca-d6ec4bc783dc","_cell_guid":"0eaa1ac5-e138-4b9e-9937-8824ccbd7b82","trusted":true}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n# Load dataset\nfile_path = '/kaggle/working/train_processed.csv'\ndata = pd.read_csv(file_path)\n\n# Encode severity labels\ndata['severity_encoded'] = data['severity'].map({\n    'Normal/Mild': 0,\n    'Moderate': 1,\n    'Severe': 2\n})\n\n# Scale the 'x' and 'y' coordinates using StandardScaler\nscaler = StandardScaler()\ndata[['x', 'y']] = scaler.fit_transform(data[['x', 'y']])\n\n# Split the data into train and test sets\ntrain_data, test_data = train_test_split(data, test_size=0.2, stratify=data['severity_encoded'], random_state=42)\n\n# Display the first few rows of the train set\ntrain_data.head()","metadata":{"_uuid":"2d9e2e6b-4b93-4725-9530-1d16532432ad","_cell_guid":"9f1366fe-e6d4-4b62-9f9b-e65a3f7bd5e1","collapsed":false,"execution":{"iopub.status.busy":"2024-09-11T18:34:18.144932Z","iopub.status.idle":"2024-09-11T18:34:18.145430Z","shell.execute_reply.started":"2024-09-11T18:34:18.145173Z","shell.execute_reply":"2024-09-11T18:34:18.145199Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load DICOM Images for Training & Testing","metadata":{"_uuid":"975df8ca-dc17-4263-910b-bd76007b8174","_cell_guid":"1659942c-9121-4519-803e-4dd50453b563","trusted":true}},{"cell_type":"code","source":"IMG_SIZE = (224, 224)\n\ndef load_and_preprocess_dicom(image_path):\n    try:\n        dicom = pydicom.dcmread(image_path)\n        img = dicom.pixel_array\n        img = cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX)\n        if len(img.shape) == 2:\n            img = np.stack([img] * 3, axis=-1)\n        img = cv2.resize(img, IMG_SIZE)\n        img = img / 255.0\n        return img\n    except Exception as e:\n        print(f\"Error loading DICOM image: {image_path}, Error: {e}\")\n        return None\n\n# Prepare training and test images\ntrain_images = []\nfor path in train_data['image_path']:\n    img = load_and_preprocess_dicom(path)\n    if img is not None:\n        train_images.append(img)\n\ntest_images = []\nfor path in test_data['image_path']:\n    img = load_and_preprocess_dicom(path)\n    if img is not None:\n        test_images.append(img)\n\ntrain_images = np.array(train_images)\ntest_images = np.array(test_images)\n\ntrain_tabular = train_data[['x', 'y']].values\ntest_tabular = test_data[['x', 'y']].values\n\ntrain_labels = train_data['severity_encoded'].values\ntest_labels = test_data['severity_encoded'].values","metadata":{"_uuid":"e0442dbb-82fe-4e06-8f61-a248c89bdb6d","_cell_guid":"76703175-e7b1-4815-80f2-07625ab22615","collapsed":false,"execution":{"iopub.status.busy":"2024-09-11T18:34:18.146986Z","iopub.status.idle":"2024-09-11T18:34:18.147499Z","shell.execute_reply.started":"2024-09-11T18:34:18.147235Z","shell.execute_reply":"2024-09-11T18:34:18.147263Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Setup for Combined Image and Tabular Data","metadata":{"_uuid":"fd5aeb8f-cec4-4634-8d71-06c49273ab2e","_cell_guid":"52c2ec4f-ca39-4100-9db2-809b6b698860","trusted":true}},{"cell_type":"code","source":"from tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras import layers, models\n\nefficientnet_input = layers.Input(shape=(224, 224, 3))\nefficientnet = EfficientNetB0(include_top=False, input_tensor=efficientnet_input, weights='imagenet')\nefficientnet.trainable = False\n\nx = layers.GlobalAveragePooling2D()(efficientnet.output)\n\ntabular_input = layers.Input(shape=(2,))\ny = layers.Dense(128, activation='relu')(tabular_input)\ny = layers.Dense(64, activation='relu')(y)\n\ncombined = layers.Concatenate()([x, y])\noutput = layers.Dense(3, activation='softmax')(combined)\n\nmodel = models.Model(inputs=[efficientnet_input, tabular_input], outputs=output)\n\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])","metadata":{"_uuid":"07a34853-d2e3-4325-ad8f-65d47bd2956f","_cell_guid":"9c95195a-3de6-467f-813a-7684fe35e601","collapsed":false,"execution":{"iopub.status.busy":"2024-09-11T18:34:18.148863Z","iopub.status.idle":"2024-09-11T18:34:18.149367Z","shell.execute_reply.started":"2024-09-11T18:34:18.149109Z","shell.execute_reply":"2024-09-11T18:34:18.149136Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Early Stopping & Training the Model","metadata":{"_uuid":"3114f458-a121-4a34-9c26-53dc01c63b29","_cell_guid":"a8ec3df1-dfc3-49be-b2b1-e4b1e7ecd73d","trusted":true}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\n\n# EarlyStopping callback\nearly_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n\n# Train the model with data generators\nhistory = model.fit(\n    train_gen,\n    validation_data=val_gen,\n    epochs=20,\n    callbacks=[early_stopping], \n    verbose=2\n)\n\n# Evaluate the model\ntest_loss, test_acc = model.evaluate(val_gen)\nprint(f\"Test accuracy: {test_acc}\")","metadata":{"_uuid":"5740d1e3-e320-4883-b851-c372b4e327e8","_cell_guid":"bcd72580-03c6-4985-a5a1-995540c219af","collapsed":false,"execution":{"iopub.status.busy":"2024-09-11T18:34:18.150393Z","iopub.status.idle":"2024-09-11T18:34:18.150911Z","shell.execute_reply.started":"2024-09-11T18:34:18.150642Z","shell.execute_reply":"2024-09-11T18:34:18.150669Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}